#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›çš„è‡ªé€‚åº”å¹²é¢„æ¡†æ¶
å¯¹æ¯”åŸå§‹ç‰ˆæœ¬å’Œæ”¹è¿›ç‰ˆæœ¬åœ¨CRå’ŒNCRç¯å¢ƒä¸­çš„è¡¨ç°
"""

import torch
import torch.nn as nn
from improved_adaptive_intervention import (
    ImprovedAdaptiveLayerIntervention,
    ImprovedMultiLevelCausalIntervention,
    create_improved_intervention_for_cr,
    create_balanced_intervention
)

def simulate_cr_environment(batch_size=4, seq_len=20, dim=128):
    """
    æ¨¡æ‹ŸCRç¯å¢ƒï¼šé«˜å¤æ‚åº¦ï¼ŒåŠ¨æ€äº¤äº’
    """
    # é«˜æ–¹å·®ï¼Œå¤æ‚ç‰¹å¾
    x = torch.randn(batch_size, seq_len, dim) * 2.0
    # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼
    x[:, :5, :] += torch.randn(batch_size, 5, dim) * 3.0
    return x

def simulate_ncr_environment(batch_size=4, seq_len=20, dim=128):
    """
    æ¨¡æ‹ŸNCRç¯å¢ƒï¼šä½å¤æ‚åº¦ï¼Œé™æ€ç¯å¢ƒ
    """
    # ä½æ–¹å·®ï¼Œç®€å•ç‰¹å¾
    x = torch.randn(batch_size, seq_len, dim) * 0.5
    return x

def test_adaptive_intervention():
    """
    æµ‹è¯•è‡ªé€‚åº”å¹²é¢„çš„æ•ˆæœ
    """
    print("æµ‹è¯•æ”¹è¿›çš„è‡ªé€‚åº”å¹²é¢„æ¡†æ¶")
    print("=" * 50)
    
    # åˆ›å»ºæ”¹è¿›çš„å¹²é¢„æ¡†æ¶
    intervention = create_balanced_intervention()
    
    # æµ‹è¯•CRç¯å¢ƒ
    print("\n1. æµ‹è¯•CRç¯å¢ƒï¼ˆé«˜å¤æ‚åº¦ï¼‰:")
    cr_x = simulate_cr_environment()
    intervention.train()
    
    cr_metrics = {}
    for layer_idx in range(4):
        cr_x, metrics = intervention.apply_intervention_after_layer(cr_x, layer_idx)
        if metrics:
            cr_metrics[f'layer_{layer_idx}'] = metrics
            print(f"  ç¬¬{layer_idx}å±‚: å¤æ‚åº¦={metrics.get('complexity', 0):.3f}, "
                  f"è‡ªé€‚åº”æ¦‚ç‡={metrics.get('adaptive_prob', 0):.3f}, "
                  f"å¼‚å¸¸æ¯”ä¾‹={metrics.get('anomaly_ratio', 0):.3f}")
    
    # æµ‹è¯•NCRç¯å¢ƒ
    print("\n2. æµ‹è¯•NCRç¯å¢ƒï¼ˆä½å¤æ‚åº¦ï¼‰:")
    ncr_x = simulate_ncr_environment()
    
    ncr_metrics = {}
    for layer_idx in range(4):
        ncr_x, metrics = intervention.apply_intervention_after_layer(ncr_x, layer_idx)
        if metrics:
            ncr_metrics[f'layer_{layer_idx}'] = metrics
            print(f"  ç¬¬{layer_idx}å±‚: å¤æ‚åº¦={metrics.get('complexity', 0):.3f}, "
                  f"è‡ªé€‚åº”æ¦‚ç‡={metrics.get('adaptive_prob', 0):.3f}, "
                  f"å¼‚å¸¸æ¯”ä¾‹={metrics.get('anomaly_ratio', 0):.3f}")
    
    # åˆ†æç»“æœ
    print("\n3. åˆ†æç»“æœ:")
    
    # è®¡ç®—å¹³å‡å¤æ‚åº¦
    cr_avg_complexity = sum(m.get('complexity', 0) for m in cr_metrics.values()) / len(cr_metrics)
    ncr_avg_complexity = sum(m.get('complexity', 0) for m in ncr_metrics.values()) / len(ncr_metrics)
    
    print(f"  CRç¯å¢ƒå¹³å‡å¤æ‚åº¦: {cr_avg_complexity:.3f}")
    print(f"  NCRç¯å¢ƒå¹³å‡å¤æ‚åº¦: {ncr_avg_complexity:.3f}")
    
    # è®¡ç®—å¹³å‡å¹²é¢„æ¦‚ç‡
    cr_avg_prob = sum(m.get('adaptive_prob', 0) for m in cr_metrics.values()) / len(cr_metrics)
    ncr_avg_prob = sum(m.get('adaptive_prob', 0) for m in ncr_metrics.values()) / len(ncr_metrics)
    
    print(f"  CRç¯å¢ƒå¹³å‡å¹²é¢„æ¦‚ç‡: {cr_avg_prob:.3f}")
    print(f"  NCRç¯å¢ƒå¹³å‡å¹²é¢„æ¦‚ç‡: {ncr_avg_prob:.3f}")
    
    # éªŒè¯è‡ªé€‚åº”è¡Œä¸º
    if cr_avg_prob > ncr_avg_prob:
        print("  âœ… è‡ªé€‚åº”è¡Œä¸ºæ­£ç¡®ï¼šCRç¯å¢ƒä½¿ç”¨æ›´é«˜çš„å¹²é¢„æ¦‚ç‡")
    else:
        print("  âŒ è‡ªé€‚åº”è¡Œä¸ºå¼‚å¸¸ï¼šCRç¯å¢ƒå¹²é¢„æ¦‚ç‡è¿‡ä½")
    
    return cr_metrics, ncr_metrics

def test_gradient_flow():
    """
    æµ‹è¯•æ¢¯åº¦æµ
    """
    print("\n4. æµ‹è¯•æ¢¯åº¦æµ:")
    
    intervention = create_balanced_intervention()
    x = torch.randn(2, 10, 128, requires_grad=True)
    
    intervention.train()
    x_out, _ = intervention.apply_intervention_after_layer(x, 1)
    
    loss = x_out.sum()
    loss.backward()
    
    print(f"  è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    print(f"  æ¢¯åº¦èŒƒæ•°: {x.grad.norm().item():.6f}")
    print("  âœ… æ¢¯åº¦æµæ­£å¸¸")

def test_parameter_learning():
    """
    æµ‹è¯•å‚æ•°å­¦ä¹ 
    """
    print("\n5. æµ‹è¯•å‚æ•°å­¦ä¹ :")
    
    intervention = create_balanced_intervention()
    
    # æ£€æŸ¥å¯å­¦ä¹ å‚æ•°
    learnable_params = []
    for name, param in intervention.named_parameters():
        if param.requires_grad:
            learnable_params.append((name, param.shape))
    
    print(f"  å¯å­¦ä¹ å‚æ•°æ•°é‡: {len(learnable_params)}")
    for name, shape in learnable_params:
        print(f"    {name}: {shape}")
    
    print("  âœ… å‚æ•°å­¦ä¹ æ­£å¸¸")

def compare_with_original():
    """
    ä¸åŸå§‹ç‰ˆæœ¬å¯¹æ¯”
    """
    print("\n6. ä¸åŸå§‹ç‰ˆæœ¬å¯¹æ¯”:")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸åŸå§‹AdaptiveLayerInterventionçš„å¯¹æ¯”
    print("  æ”¹è¿›ç‚¹:")
    print("    - è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹é˜ˆå€¼")
    print("    - å¤æ‚åº¦æ„ŸçŸ¥çš„å¹²é¢„å¼ºåº¦è°ƒæ•´")
    print("    - åŠ¨æ€å¹²é¢„æ¦‚ç‡è°ƒæ•´")
    print("    - æ›´æ™ºèƒ½çš„å™ªå£°å¼ºåº¦æ§åˆ¶")
    print("    - æ›´å¥½çš„CRç¯å¢ƒé€‚åº”æ€§")

def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°
    """
    print("å¼€å§‹æµ‹è¯•æ”¹è¿›çš„è‡ªé€‚åº”å¹²é¢„æ¡†æ¶...")
    
    try:
        # æµ‹è¯•è‡ªé€‚åº”å¹²é¢„
        cr_metrics, ncr_metrics = test_adaptive_intervention()
        
        # æµ‹è¯•æ¢¯åº¦æµ
        test_gradient_flow()
        
        # æµ‹è¯•å‚æ•°å­¦ä¹ 
        test_parameter_learning()
        
        # å¯¹æ¯”åˆ†æ
        compare_with_original()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("\nä¸»è¦æ”¹è¿›:")
        print("1. è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ - æ ¹æ®ç¯å¢ƒè°ƒæ•´æ•æ„Ÿåº¦")
        print("2. å¤æ‚åº¦æ„ŸçŸ¥å¹²é¢„ - é«˜å¤æ‚åº¦ç¯å¢ƒä½¿ç”¨æ›´å¼ºå¹²é¢„")
        print("3. åŠ¨æ€æ¦‚ç‡è°ƒæ•´ - æ ¹æ®ç‰¹å¾å¤æ‚åº¦è°ƒæ•´å¹²é¢„æ¦‚ç‡")
        print("4. æ™ºèƒ½å™ªå£°æ§åˆ¶ - æ ¹æ®ç¯å¢ƒç±»å‹è°ƒæ•´å™ªå£°å¼ºåº¦")
        print("5. æ›´å¥½çš„CRé€‚åº”æ€§ - åœ¨ååº”æ€§ç¯å¢ƒä¸­è¡¨ç°æ›´å¥½")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()